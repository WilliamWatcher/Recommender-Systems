{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "##### How to run:\n",
    "\n",
    "Run all cells up to, and including, the loading data section.\n",
    "\n",
    "Run all cells in the section belonging to the recommender you want to evaluate.\n",
    "\n",
    "\n",
    "## Required packages:\n",
    "\n",
    "##### All:\n",
    " - pandas\n",
    " - numpy\n",
    " - scikit-learn\n",
    "\n",
    "##### Collaborative Filtering:\n",
    " - pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from datetime import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all impressions from a specific user in a behavior dataframe, and add them to a new dataframe\n",
    "def get_impressions(userID, behavior_view):\n",
    "    test = []\n",
    "    for index, row in behavior_view.iterrows():\n",
    "        if row[\"User ID\"] == userID:\n",
    "            for impression in row[\"Impressions\"].split(\" \"):\n",
    "                imp = impression.split(\"-\")\n",
    "                if imp[1] == \"1\":\n",
    "                    test.append((imp[0], 1))\n",
    "                else:\n",
    "                    test.append((imp[0], 0))\n",
    "    return pd.DataFrame(test, columns=[\"News ID\", \"Response\"])\n",
    "\n",
    "\n",
    "# Get df of all user IDs\n",
    "def get_users(view1, view2):\n",
    "    return pd.merge(left=view1[\"User ID\"], right=view2[\"User ID\"], how=\"inner\", on=\"User ID\")[\"User ID\"].unique()\n",
    "\n",
    "# Get a timegated view of a behavior dataframe\n",
    "def get_view(behavior, t0, t1):\n",
    "    df = behavior[(behavior[\"Timestamp\"] >= t0) & (behavior[\"Timestamp\"] < t1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = pd.read_csv(\"data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"])\n",
    "news = pd.read_csv(\"data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\", \"Title Topics\", \"Abstract Topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_timestamp(str):\n",
    "    return datetime.strptime(str, \"%m/%d/%Y %H:%M:%S %p\").timestamp()\n",
    "\n",
    "timestamps = behavior[\"Time\"].apply(str_to_timestamp)\n",
    "behavior[\"Timestamp\"] = timestamps\n",
    "behavior.sort_values(by=\"Timestamp\")\n",
    "\n",
    "#Fill missing abstracts with placeholder\n",
    "news['Abstract'].fillna('No abstract available', inplace=True)\n",
    "\n",
    "# if there are rows with no impressions, drop them\n",
    "behavior = behavior.dropna(subset=['Impressions']) # this looses some user information, could instead manually overwrite and fill in the missing values based on the typo combining the impression and history columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Feature-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions from feature_based.ipynb\n",
    "# Comment out function calls in feature_based.ipynb (except the pd.set_option calls) before running this field\n",
    "%run feature_based.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j, news_id_to_index = create_all_item_vectors(news)\n",
    "\n",
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "\n",
    "split_ratio = 2/3\n",
    "dt = (tn-t0)/5\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    for user in users:\n",
    "        prediction = pd.DataFrame(find_top_k_articles(user, X_j, 600000, news_id_to_index, train_view), columns=[\"News ID\", \"Score\"])\n",
    "        response = get_impressions(user, test_view)\n",
    "        \n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"News ID\", how=\"inner\")\n",
    "\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "        \n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions from item_collab_filtering.ipynb\n",
    "# Comment out function calls (except for the call to initiate a spark engine) in item_collab_filtering.ipynb before running this field\n",
    "%run item_collab_filtering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratio = 2/3\n",
    "dt = (tn-t0)/10\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_collaborative_filtering_model(train_view)\n",
    "\n",
    "    # Can swap which for loop you iterate through, if iterating over all users takes too long\n",
    "    #for user in users[:500]:\n",
    "    for user in users:\n",
    "        prediction = get_top_n_recommendations(user, model, N=600000).toPandas()\n",
    "        response = get_impressions(user, test_view)\n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"news_id\", how=\"inner\")\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking against randomized scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratio = 2/3\n",
    "dt = (tn-t0)/10\n",
    "k = 5\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving timeframe\n",
      "moving timeframe\n",
      "moving timeframe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get ranked recommendations, and evaluate them for all users\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m users:\n\u001b[1;32m---> 17\u001b[0m     pred_resp \u001b[38;5;241m=\u001b[39m \u001b[43mget_impressions\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_view\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     pred_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom(pred_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mget_impressions\u001b[1;34m(userID, behavior_view)\u001b[0m\n\u001b[0;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m behavior_view\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m userID:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m impression \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImpressions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m             imp \u001b[38;5;241m=\u001b[39m impression\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\Documents\\School\\sem9\\RecSysProject\\Recommender-Systems\\local\\Lib\\site-packages\\pandas\\core\\series.py:1096\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m-> 1096\u001b[0m     \u001b[43mcheck_dict_or_set_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\Documents\\School\\sem9\\RecSysProject\\Recommender-Systems\\local\\Lib\\site-packages\\pandas\\core\\indexing.py:2765\u001b[0m, in \u001b[0;36mcheck_dict_or_set_indexers\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m   2753\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2754\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   2755\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;124;03m    bool\u001b[39;00m\n\u001b[0;32m   2757\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   2759\u001b[0m         obj\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (obj\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2762\u001b[0m     )\n\u001b[1;32m-> 2765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_dict_or_set_indexers\u001b[39m(key) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2767\u001b[0m \u001b[38;5;124;03m    Check if the indexer is or contains a dict or set, which is no longer allowed.\u001b[39;00m\n\u001b[0;32m   2768\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2770\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m   2771\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m   2772\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mset\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   2773\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "\n",
    "    # Get ranked recommendations, and evaluate them for all users\n",
    "    for user in users:\n",
    "        pred_resp = get_impressions(user, test_view)\n",
    "        pred_resp[\"Score\"] = np.random.random(pred_resp[\"Response\"].size)\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
