{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "##### How to run:\n",
    "\n",
    "Run all cells up to, and including, the loading data section.\n",
    "\n",
    "Before running the evaluation, comment out function calls, dataset loading, and test code in the files for the recommenders (feature_based.ipynb or item_collab_filtering.ipynb).\n",
    "Run all cells in the section belonging to the recommender you want to evaluate.\n",
    "\n",
    "\n",
    "## Required packages:\n",
    "\n",
    "##### All:\n",
    " - pandas\n",
    " - numpy\n",
    " - scikit-learn\n",
    "\n",
    "##### Collaborative Filtering:\n",
    " - pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from datetime import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all impressions from a specific user in a behavior dataframe, and add them to a new dataframe\n",
    "def get_impressions(userID, behavior_view):\n",
    "    test = []\n",
    "    for index, row in behavior_view.iterrows():\n",
    "        if row[\"User ID\"] == userID:\n",
    "            for impression in row[\"Impressions\"].split(\" \"):\n",
    "                imp = impression.split(\"-\")\n",
    "                if imp[1] == \"1\":\n",
    "                    test.append((imp[0], 1))\n",
    "                else:\n",
    "                    test.append((imp[0], 0))\n",
    "    return pd.DataFrame(test, columns=[\"News ID\", \"Response\"])\n",
    "\n",
    "\n",
    "# Get df of all user IDs\n",
    "def get_users(view1, view2):\n",
    "    return pd.merge(left=view1[\"User ID\"], right=view2[\"User ID\"], how=\"inner\", on=\"User ID\")[\"User ID\"].unique()\n",
    "\n",
    "# Get a timegated view of a behavior dataframe\n",
    "def get_view(behavior, t0, t1):\n",
    "    df = behavior[(behavior[\"Timestamp\"] >= t0) & (behavior[\"Timestamp\"] < t1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = pd.read_csv(\"data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"])\n",
    "news = pd.read_csv(\"data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\", \"Title Topics\", \"Abstract Topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marle\\AppData\\Local\\Temp\\ipykernel_5816\\3025606149.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  news['Abstract'].fillna('No abstract available', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def str_to_timestamp(str):\n",
    "    return datetime.strptime(str, \"%m/%d/%Y %H:%M:%S %p\").timestamp()\n",
    "\n",
    "timestamps = behavior[\"Time\"].apply(str_to_timestamp)\n",
    "behavior[\"Timestamp\"] = timestamps\n",
    "behavior.sort_values(by=\"Timestamp\")\n",
    "\n",
    "#Fill missing abstracts with placeholder\n",
    "news['Abstract'].fillna('No abstract available', inplace=True)\n",
    "\n",
    "# if there are rows with no impressions, drop them\n",
    "behavior = behavior.dropna(subset=['Impressions']) # this looses some user information, could instead manually overwrite and fill in the missing values based on the typo combining the impression and history columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Feature-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions from feature_based.ipynb\n",
    "# Comment out function calls in feature_based.ipynb (except the pd.set_option calls) before running this field\n",
    "# %run feature_based.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nX_j, news_id_to_index = create_all_item_vectors(news)\\n\\nevaluations_ndcg = []\\nevaluations_pak = []\\n\\nt0 = float(behavior[\"Timestamp\"][0])\\ntn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\\n\\nsplit_ratio = 85/100\\ndt = (tn-t0)/5\\nk = 5\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "X_j, news_id_to_index = create_all_item_vectors(news)\n",
    "\n",
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "\n",
    "split_ratio = 85/100\n",
    "dt = (tn-t0)/5\n",
    "k = 5\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nwhile t0 <= tn-dt:\\n    # Splitting into train and test views\\n    tsplit = t0 + dt*split_ratio\\n    t1 = t0 + dt\\n\\n    train_view = get_view(behavior, t0, tsplit)\\n    test_view = get_view(behavior, tsplit, t1)\\n\\n    # Finding users that have impressions in both the test and train view\\n    users = get_users(test_view, train_view)\\n\\n    for user in users:\\n        prediction = pd.DataFrame(find_top_k_articles(user, X_j, 600000, news_id_to_index, train_view), columns=[\"News ID\", \"Score\"])\\n        response = get_impressions(user, test_view)\\n        \\n        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"News ID\", how=\"inner\")\\n\\n        try:\\n            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\\n            evaluations_ndcg.append(evaluation)\\n            k_slice = pred_resp[\"Response\"].iloc[:k]\\n            evaluation = k_slice.sum()/min(len(k_slice), k)\\n            evaluations_pak.append(evaluation)\\n            #print(\"Predictions for user \" + user + \" evaluated!\")\\n        except:\\n            #print(\"Eval failed\")\\n            pass\\n        \\n\\n    print(\"moving timeframe\")\\n    t0 = tsplit\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop that runs the sliding window\n",
    "\"\"\"\"\"\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    for user in users:\n",
    "        prediction = pd.DataFrame(find_top_k_articles(user, X_j, 600000, news_id_to_index, train_view), columns=[\"News ID\", \"Score\"])\n",
    "        response = get_impressions(user, test_view)\n",
    "        \n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"News ID\", how=\"inner\")\n",
    "\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "        \n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nprint(\"nDCG:\")\\nprint(sum(evaluations_ndcg)/len(evaluations_ndcg))\\nprint(evaluations_ndcg)\\nprint(\"Precision at K:\")\\nprint(sum(evaluations_pak)/len(evaluations_pak))\\nprint(evaluations_pak)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(evaluations_ndcg)\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))\n",
    "print(evaluations_pak)\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'read' from 'nbformat' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing functions from item_collab_filtering.ipynb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Comment out function calls (except for the call to initiate a spark engine) in item_collab_filtering.ipynb before running this field\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_collab_filtering.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marle\\OneDrive - NTNU\\NTNU_2023_2024\\Anbefaling systemer\\Gruppearbeid\\Recommender-Systems\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\marle\\OneDrive - NTNU\\NTNU_2023_2024\\Anbefaling systemer\\Gruppearbeid\\Recommender-Systems\\venv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:737\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 737\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marle\\OneDrive - NTNU\\NTNU_2023_2024\\Anbefaling systemer\\Gruppearbeid\\Recommender-Systems\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3002\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   3000\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m prepended_to_syspath(dname):\n\u001b[0;32m   3001\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3002\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m get_cells():\n\u001b[0;32m   3003\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   3004\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n",
      "File \u001b[1;32mc:\\Users\\marle\\OneDrive - NTNU\\NTNU_2023_2024\\Anbefaling systemer\\Gruppearbeid\\Recommender-Systems\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2990\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy.<locals>.get_cells\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2988\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"generator for sequence of code blocks to run\"\"\"\u001b[39;00m\n\u001b[0;32m   2989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fname\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2990\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnbformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read\n\u001b[0;32m   2991\u001b[0m     nb \u001b[38;5;241m=\u001b[39m read(fname, as_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m   2992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nb\u001b[38;5;241m.\u001b[39mcells:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'read' from 'nbformat' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Importing functions from item_collab_filtering.ipynb\n",
    "# Comment out function calls (except for the call to initiate a spark engine) in item_collab_filtering.ipynb before running this field\n",
    "%run item_collab_filtering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratio = 85/100\n",
    "dt = (tn-t0)/5\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_collaborative_filtering_model(train_view)\n",
    "\n",
    "    # Can swap which for loop you iterate through, if iterating over all users takes too long\n",
    "    #for user in users[:500]:\n",
    "    for user in users:\n",
    "        spark_prediction = get_top_n_recommendations(user, model, N=600000)\n",
    "        prediction = spark_prediction.toPandas()\n",
    "        \n",
    "        spark_prediction.unpersist()\n",
    "        \n",
    "        response = get_impressions(user, test_view)\n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"news_id\", how=\"inner\")\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    spark.catalog.clearCache()\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less time efficient hack to solve memory issues:\n",
    "# Comment out the sparksession creator in item_collab_filttering.ipynb, then run this instead of the block above\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    # Can swap which for loop you iterate through, if iterating over all users takes too long\n",
    "    #for user in users[:500]:\n",
    "    for user in users[:100]:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"MINDItemBasedFiltering\") \\\n",
    "            .config(\"spark.executor.memory\", \"2g\") \\\n",
    "            .config(\"spark.driver.memory\", \"2g\") \\\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Train the model\n",
    "        model = train_collaborative_filtering_model(train_view)\n",
    "\n",
    "        prediction = get_top_n_recommendations(user, model, N=600000).toPandas()\n",
    "        response = get_impressions(user, test_view)\n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"news_id\", how=\"inner\")\n",
    "\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    spark.catalog.clearCache()\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking against randomized scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratio = 85/100\n",
    "dt = (tn-t0)/5\n",
    "k = 5\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "\n",
    "    # Get ranked recommendations, and evaluate them for all users\n",
    "    for user in users:\n",
    "        pred_resp = get_impressions(user, test_view)\n",
    "        pred_resp[\"Score\"] = np.random.random(pred_resp[\"Response\"].size)\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratios = [i/100 for i in range(50,100)]\n",
    "dts = [(tn-t0)/n for n in range(5,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "sizes = []\n",
    "for dt in dts:\n",
    "    aggregate_counts = []\n",
    "    aggregate_sizes = []\n",
    "    for split_ratio in split_ratios:\n",
    "        user_counts = []\n",
    "        train_sizes = []\n",
    "        start_time = t0\n",
    "        while start_time <= tn-dt:\n",
    "            tsplit = start_time + dt*split_ratio\n",
    "            t1 = start_time + dt\n",
    "            train_view = get_view(behavior, start_time, tsplit)\n",
    "            test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "            user_count = len(get_users(train_view, test_view))\n",
    "            user_counts.append(user_count)\n",
    "\n",
    "            train_size = len(train_view)\n",
    "            train_sizes.append(train_size)\n",
    "\n",
    "            start_time = tsplit\n",
    "        aggregate_counts.append(sum(user_counts)/len(user_counts))\n",
    "        aggregate_sizes.append(sum(train_sizes)/len(train_sizes))\n",
    "    counts.append(aggregate_counts)\n",
    "    sizes.append(aggregate_sizes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i/100 for i in range(50,100)]\n",
    "plt.figure(0)\n",
    "plt.plot(x, counts[0])\n",
    "plt.ylabel(\"User count\")\n",
    "plt.xlabel(\"Split ratio\")\n",
    "plt.scatter((35+50)/100, counts[0][35])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(x, sizes[0])\n",
    "plt.ylabel(\"Train set size\")\n",
    "plt.xlabel(\"Split ratio\")\n",
    "plt.scatter((35+50)/100, sizes[0][35])\n",
    "plt.show()\n",
    "\n",
    "print(counts[0].index(max(counts[0][80-50:90-50])))\n",
    "print(counts[0][35])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
