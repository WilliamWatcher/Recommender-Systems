{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from datetime import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all impressions for a user and add them to a df\n",
    "def get_impressions(userID, behavior_view):\n",
    "    test = []\n",
    "    for index, row in behavior_view.iterrows():\n",
    "        if row[\"User ID\"] == userID:\n",
    "            for impression in row[\"Impressions\"].split(\" \"):\n",
    "                imp = impression.split(\"-\")\n",
    "                if imp[1] == \"1\":\n",
    "                    test.append((imp[0], 1))\n",
    "                else:\n",
    "                    test.append((imp[0], 0))\n",
    "    return pd.DataFrame(test, columns=[\"News ID\", \"Response\"])\n",
    "\n",
    "\n",
    "# Join response to our predictions in order to sort them before evaluation\n",
    "def create_evaluation_data(scored_data, userID):\n",
    "    return scored_data.join(get_impressions(userID).set_index(\"News ID\"), how=\"inner\", on=\"News ID\")\n",
    "\n",
    "\n",
    "# Get df of all user IDs\n",
    "def get_users(view1, view2):\n",
    "    return view1[\"User ID\"].join(view2[\"User ID\"].set_index(\"User ID\"), on=\"User ID\", how=\"inner\").unique()\n",
    "\n",
    "#userList = __get_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_view(behavior, t0, t1):\n",
    "    df = behavior[(behavior[\"Time\"] >= t0) & (behavior[\"Time\"] < t1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = pd.read_csv(\"data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"])\n",
    "news = pd.read_csv(\"data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\", \"Title Topics\", \"Abstract Topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_timestamp(str):\n",
    "    return datetime.strptime(str, \"%m/%d/%Y %H:%M:%S %p\").timestamp()\n",
    "\n",
    "timestamps = behavior[\"Time\"].apply(str_to_timestamp)\n",
    "behavior[\"Time\"] = timestamps\n",
    "behavior.sort_values(by=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Feature-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run feature_based.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j, news_id_to_index = create_all_item_vectors(news, behavior)\n",
    "\n",
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Time\"][0])\n",
    "tn = float(behavior[\"Time\"][len(behavior[\"Time\"])-1])\n",
    "split_ratio = 2/3\n",
    "dt = (tn-t0)/10\n",
    "k = 5\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    for i, row in users.iterrows():\n",
    "        user = row[\"User ID\"]\n",
    "        x_i = create_user_vector(user, X_j, train_view)\n",
    "        prediction = pd.DataFrame(find_top_k_articles(user, X_j, 10, news_id_to_index), columns=[\"News ID\", \"Score\"])\n",
    "        response = get_impressions(user, test_view)\n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"News ID\", how=\"inner\")\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run item_collab_filtering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Time\"][0])\n",
    "tn = float(behavior[\"Time\"][len(behavior[\"Time\"])-1])\n",
    "split_ratio = 2/3\n",
    "dt = (tn-t0)/10\n",
    "k = 5\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    model = train_collaborative_filtering_model(train_view)\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    for i, row in users.iterrows():\n",
    "        user = row[\"User ID\"]\n",
    "        prediction = get_top_n_recommendations(user, model, N=10).toPandas()\n",
    "        response = get_impressions(user, test_view)\n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"news_id\", how=\"inner\")\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
