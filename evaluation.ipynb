{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "##### How to run:\n",
    "\n",
    "Run all cells up to, and including, the loading data section.\n",
    "\n",
    "Before running the evaluation, comment out function calls, dataset loading, and test code in the files for the recommenders (feature_based.ipynb or item_collab_filtering.ipynb).\n",
    "Run all cells in the section belonging to the recommender you want to evaluate.\n",
    "\n",
    "\n",
    "## Required packages:\n",
    "\n",
    "##### All:\n",
    " - pandas\n",
    " - numpy\n",
    " - scikit-learn\n",
    "\n",
    "##### Collaborative Filtering:\n",
    " - pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from datetime import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all impressions from a specific user in a behavior dataframe, and add them to a new dataframe\n",
    "def get_impressions(userID, behavior_view):\n",
    "    test = []\n",
    "    for index, row in behavior_view.iterrows():\n",
    "        if row[\"User ID\"] == userID:\n",
    "            for impression in row[\"Impressions\"].split(\" \"):\n",
    "                imp = impression.split(\"-\")\n",
    "                if imp[1] == \"1\":\n",
    "                    test.append((imp[0], 1))\n",
    "                else:\n",
    "                    test.append((imp[0], 0))\n",
    "    return pd.DataFrame(test, columns=[\"News ID\", \"Response\"])\n",
    "\n",
    "\n",
    "# Get df of all user IDs\n",
    "def get_users(view1, view2):\n",
    "    return pd.merge(left=view1[\"User ID\"], right=view2[\"User ID\"], how=\"inner\", on=\"User ID\")[\"User ID\"].unique()\n",
    "\n",
    "# Get a timegated view of a behavior dataframe\n",
    "def get_view(behavior, t0, t1):\n",
    "    df = behavior[(behavior[\"Timestamp\"] >= t0) & (behavior[\"Timestamp\"] < t1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = pd.read_csv(\"data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"])\n",
    "news = pd.read_csv(\"data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\", \"Title Topics\", \"Abstract Topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_timestamp(str):\n",
    "    return datetime.strptime(str, \"%m/%d/%Y %H:%M:%S %p\").timestamp()\n",
    "\n",
    "timestamps = behavior[\"Time\"].apply(str_to_timestamp)\n",
    "behavior[\"Timestamp\"] = timestamps\n",
    "behavior.sort_values(by=\"Timestamp\")\n",
    "\n",
    "#Fill missing abstracts with placeholder\n",
    "news['Abstract'].fillna('No abstract available', inplace=True)\n",
    "\n",
    "# if there are rows with no impressions, drop them\n",
    "behavior = behavior.dropna(subset=['Impressions']) # this looses some user information, could instead manually overwrite and fill in the missing values based on the typo combining the impression and history columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Feature-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions from feature_based.ipynb\n",
    "# Comment out function calls in feature_based.ipynb (except the pd.set_option calls) before running this field\n",
    "%run feature_based.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j, news_id_to_index = create_all_item_vectors(news)\n",
    "\n",
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "\n",
    "split_ratio = 85/100\n",
    "dt = (tn-t0)/5\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    for user in users:\n",
    "        prediction = pd.DataFrame(find_top_k_articles(user, X_j, 600000, news_id_to_index, train_view), columns=[\"News ID\", \"Score\"])\n",
    "        response = get_impressions(user, test_view)\n",
    "        \n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"News ID\", how=\"inner\")\n",
    "\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "        \n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(evaluations_ndcg)\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))\n",
    "print(evaluations_pak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions from item_collab_filtering.ipynb\n",
    "# Comment out function calls (except for the call to initiate a spark engine) in item_collab_filtering.ipynb before running this field\n",
    "%run item_collab_filtering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratio = 85/100\n",
    "dt = (tn-t0)/5\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_collaborative_filtering_model(train_view)\n",
    "\n",
    "    # Can swap which for loop you iterate through, if iterating over all users takes too long\n",
    "    #for user in users[:500]:\n",
    "    for user in users:\n",
    "        spark_prediction = get_top_n_recommendations(user, model, N=600000)\n",
    "        prediction = spark_prediction.toPandas()\n",
    "        \n",
    "        spark_prediction.unpersist()\n",
    "        \n",
    "        response = get_impressions(user, test_view)\n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"news_id\", how=\"inner\")\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    spark.catalog.clearCache()\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less time efficient hack to solve memory issues:\n",
    "# Comment out the sparksession creator in item_collab_filttering.ipynb, then run this instead of the block above\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "    # Can swap which for loop you iterate through, if iterating over all users takes too long\n",
    "    #for user in users[:500]:\n",
    "    for user in users[:100]:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"MINDItemBasedFiltering\") \\\n",
    "            .config(\"spark.executor.memory\", \"2g\") \\\n",
    "            .config(\"spark.driver.memory\", \"2g\") \\\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Train the model\n",
    "        model = train_collaborative_filtering_model(train_view)\n",
    "\n",
    "        prediction = get_top_n_recommendations(user, model, N=600000).toPandas()\n",
    "        response = get_impressions(user, test_view)\n",
    "        pred_resp = prediction.join(response.set_index(\"News ID\"), on=\"news_id\", how=\"inner\")\n",
    "\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    spark.catalog.clearCache()\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking against randomized scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_ndcg = []\n",
    "evaluations_pak = []\n",
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratio = 85/100\n",
    "dt = (tn-t0)/5\n",
    "k = 5\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that runs the sliding window\n",
    "\n",
    "while t0 <= tn-dt:\n",
    "    # Splitting into train and test views\n",
    "    tsplit = t0 + dt*split_ratio\n",
    "    t1 = t0 + dt\n",
    "\n",
    "    train_view = get_view(behavior, t0, tsplit)\n",
    "    test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "    # Finding users that have impressions in both the test and train view\n",
    "    users = get_users(test_view, train_view)\n",
    "\n",
    "\n",
    "    # Get ranked recommendations, and evaluate them for all users\n",
    "    for user in users:\n",
    "        pred_resp = get_impressions(user, test_view)\n",
    "        pred_resp[\"Score\"] = np.random.random(pred_resp[\"Response\"].size)\n",
    "        try:\n",
    "            evaluation = ndcg_score(np.array([pred_resp[\"Response\"].to_numpy()]), np.array([pred_resp[\"Score\"].to_numpy()]))\n",
    "            evaluations_ndcg.append(evaluation)\n",
    "            k_slice = pred_resp[\"Response\"].iloc[:k]\n",
    "            evaluation = k_slice.sum()/min(len(k_slice), k)\n",
    "            evaluations_pak.append(evaluation)\n",
    "            #print(\"Predictions for user \" + user + \" evaluated!\")\n",
    "        except:\n",
    "            #print(\"Eval failed\")\n",
    "            pass\n",
    "\n",
    "    print(\"moving timeframe\")\n",
    "    t0 = tsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nDCG:\")\n",
    "print(sum(evaluations_ndcg)/len(evaluations_ndcg))\n",
    "print(\"Precision at K:\")\n",
    "print(sum(evaluations_pak)/len(evaluations_pak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = float(behavior[\"Timestamp\"][0])\n",
    "tn = float(behavior[\"Timestamp\"][len(behavior[\"Timestamp\"])-1])\n",
    "split_ratios = [i/100 for i in range(50,100)]\n",
    "dts = [(tn-t0)/n for n in range(5,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "sizes = []\n",
    "for dt in dts:\n",
    "    aggregate_counts = []\n",
    "    aggregate_sizes = []\n",
    "    for split_ratio in split_ratios:\n",
    "        user_counts = []\n",
    "        train_sizes = []\n",
    "        start_time = t0\n",
    "        while start_time <= tn-dt:\n",
    "            tsplit = start_time + dt*split_ratio\n",
    "            t1 = start_time + dt\n",
    "            train_view = get_view(behavior, start_time, tsplit)\n",
    "            test_view = get_view(behavior, tsplit, t1)\n",
    "\n",
    "            user_count = len(get_users(train_view, test_view))\n",
    "            user_counts.append(user_count)\n",
    "\n",
    "            train_size = len(train_view)\n",
    "            train_sizes.append(train_size)\n",
    "\n",
    "            start_time = tsplit\n",
    "        aggregate_counts.append(sum(user_counts)/len(user_counts))\n",
    "        aggregate_sizes.append(sum(train_sizes)/len(train_sizes))\n",
    "    counts.append(aggregate_counts)\n",
    "    sizes.append(aggregate_sizes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i/100 for i in range(50,100)]\n",
    "plt.figure(0)\n",
    "plt.plot(x, counts[0])\n",
    "plt.ylabel(\"User count\")\n",
    "plt.xlabel(\"Split ratio\")\n",
    "plt.scatter((35+50)/100, counts[0][35])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(x, sizes[0])\n",
    "plt.ylabel(\"Train set size\")\n",
    "plt.xlabel(\"Split ratio\")\n",
    "plt.scatter((35+50)/100, sizes[0][35])\n",
    "plt.show()\n",
    "\n",
    "print(counts[0].index(max(counts[0][80-50:90-50])))\n",
    "print(counts[0][35])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
