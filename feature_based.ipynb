{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the dataset\n",
    "\n",
    "behavior = pd.read_csv(\"data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"])\n",
    "news = pd.read_csv(\"data/MINDsmall_dev/news.tsv\", sep=\"\\t\", header=None, names=[\"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\", \"Title Topics\", \"Abstract Topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lifestyle': 0.010714285714285714, 'lifestyleroyals': 0.028571428571428574, 'health': 0.005357142857142857, 'fitness': 0.014285714285714287, 'finance': 0.008027597402597402, 'finance-companies': 0.021406926406926405, 'weather': 0.0026785714285714286, 'weathertopstories': 0.0071428571428571435, 'news': 0.004871794871794871, 'newscrime': 0.0047619047619047615, 'music': 0.001530612244897959, 'music-celebrity': 0.004081632653061225, 'newsus': 0.005372405372405373, 'newsscienceandtechnology': 0.002857142857142857, 'travel': 0.0008928571428571428, 'travelnews': 0.0023809523809523807, 'sports': 0.02142857142857143, 'football_nfl': 0.05714285714285715}\n"
     ]
    }
   ],
   "source": [
    "#Find category and subcategory of an article from an articleID\n",
    "def article_tags(articleID):\n",
    "    tags = []\n",
    "    article = news.loc[news[\"News ID\"] == articleID]\n",
    "    tags.append(article[\"Category\"].values[0])\n",
    "    tags.append(article[\"Subcategory\"].values[0])\n",
    "    return tags[0], tags[1]\n",
    "\n",
    "def add_feature_to_xi(x_i, feature, weight):\n",
    "    if feature in x_i:\n",
    "        x_i[feature] += weight\n",
    "    else:\n",
    "        x_i[feature] = weight\n",
    "\n",
    "#Create user vector from a userID\n",
    "def create_x_i(userID):\n",
    "    x_i = {}\n",
    "\n",
    "    # Weights\n",
    "    w_cat = 0.3  \n",
    "    w_subcat = 0.8\n",
    "    w_history = 0.5\n",
    "    w_impression = 1.0\n",
    "\n",
    "    article_count = 0\n",
    "\n",
    "    # Add categories and subcategories\n",
    "    for index, row in behavior.iterrows():\n",
    "        if row[\"User ID\"] == userID:\n",
    "            # Adding features from articles in history\n",
    "            history_count = 0\n",
    "            # Assuming history starting with oldest click - must be checked, if not, remove reverse operator\n",
    "            history = row[\"History\"].split(\" \")\n",
    "            history.reverse()\n",
    "            for articleID in history:\n",
    "                history_count += 1\n",
    "                article_count += 1\n",
    "\n",
    "                category, subcategory = article_tags(articleID)\n",
    "                add_feature_to_xi(x_i, category, w_cat*w_history/history_count)\n",
    "                add_feature_to_xi(x_i, subcategory, w_subcat*w_history/history_count)\n",
    "\n",
    "                # TODO Add entities from articles in history here. We are already iterating through all articles in a users history, need method for extracting entities\n",
    "                # entities = find_entities(articleID)   -   to be implemented\n",
    "                # for entity in entities:\n",
    "                #   add_feature_to_xi(x_i, entity, w_entity*w_history)\n",
    "    \n",
    "            # Adding features from articles in impressions\n",
    "            for impression in row[\"Impressions\"].split(\" \"):\n",
    "                if impression.split(\"-\")[1] == \"1\":\n",
    "                    article_count += 1\n",
    "\n",
    "                    category, subcategory = article_tags(impression.split(\"-\")[0])\n",
    "                    add_feature_to_xi(x_i, category, w_cat*w_impression)\n",
    "                    add_feature_to_xi(x_i, subcategory, w_subcat*w_impression)\n",
    "\n",
    "                    # TODO Add entities from articles in impressions here. We are already iterating through all articles in a users impressions, need method for extracting entities\n",
    "                    # entities = find_entities(articleID)   -   to be implemented\n",
    "                    # for entity in entities:\n",
    "                    #   add_feature_to_xi(x_i, entity, w_entity*w_history)\n",
    "\n",
    "    \n",
    "    # Normalizing\n",
    "    for trait, score in x_i.items():\n",
    "        x_i[trait] = score/article_count\n",
    "\n",
    "    return x_i\n",
    "\n",
    "exID = behavior[\"User ID\"].values[1]\n",
    "\n",
    "x_i = create_x_i(exID)\n",
    "print(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'News ID': 'N18955', 'health': 1.0, 'medical': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def create_all_x_j():\n",
    "    all_x_j = []\n",
    "    for index, row in news.iterrows():\n",
    "        x_j = {}\n",
    "        x_j[\"News ID\"] = row[\"News ID\"]\n",
    "        x_j[row[\"Category\"]] = 1.0\n",
    "        x_j[row[\"Subcategory\"]] = 1.0\n",
    "        all_x_j.append(x_j)\n",
    "\n",
    "        # TODO add entities to item vector x_j\n",
    "\n",
    "    return all_x_j\n",
    "\n",
    "X_j = create_all_x_j()\n",
    "print(X_j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('N2073', 0.05714285714285715), ('N16587', 0.05714285714285715), ('N29120', 0.05714285714285715), ('N64723', 0.05714285714285715), ('N27190', 0.05714285714285715), ('N9035', 0.05714285714285715), ('N41277', 0.05714285714285715), ('N42921', 0.05714285714285715), ('N19888', 0.05714285714285715), ('N27334', 0.05714285714285715)]\n"
     ]
    }
   ],
   "source": [
    "def score(userID):\n",
    "    x_i = create_x_i(userID)\n",
    "    X_j = create_all_x_j()\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for x_j in X_j:\n",
    "        for c, s in x_j.items():\n",
    "            article_score = 0\n",
    "            if c in x_i.keys():\n",
    "                article_score += s * x_i[c]\n",
    "        scores[x_j[\"News ID\"]] = article_score\n",
    "    \n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(score(exID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
