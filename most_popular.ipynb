{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most-popular method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# load the dataset\n",
    "\n",
    "behaviors_dev_df = pd.read_csv(\"data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"])\n",
    "news_dev_df = pd.read_csv(\"data/MINDsmall_dev/news.tsv\", sep=\"\\t\", header=None, names=[\"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\", \"Title Topics\", \"Abstract Topics\"])\n",
    "\n",
    "behaviors_train_df = pd.read_csv(\"data/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"Impression ID\", \"User ID\", \"Time\", \"History\", \"Impressions\"])\n",
    "news_train_df = pd.read_csv(\"data/MINDsmall_train/news.tsv\", sep=\"\\t\", header=None, names=[\"News ID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"Title Entities\", \"Abstract Entities\", \"Title Topics\", \"Abstract Topics\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing abstracts with placeholder\n",
    "news_dev_df['Abstract'].fillna('No abstract available', inplace=True)\n",
    "news_train_df['Abstract'].fillna('No abstract available', inplace=True)\n",
    "\n",
    "\n",
    "# if there are rows with no impressions, drop them\n",
    "behaviors_dev_df = behaviors_dev_df.dropna(subset=['Impressions']) # this looses some user information, could instead manually overwrite and fill in the missing values based on the typo combining the impression and history columns\n",
    "behaviors_train_df = behaviors_train_df.dropna(subset=['Impressions']) # this looses some user information, could instead manually overwrite and fill in the missing values based on the typo combining the impression and history columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_popular_news(behaviors_df, n):\n",
    "    # Convert 'Time' column to datetime\n",
    "    behaviors_df['Time'] = pd.to_datetime(behaviors_df['Time'])\n",
    "    \n",
    "    # Find the most recent interaction time\n",
    "    most_recent_interaction_time = behaviors_df['Time'].max()\n",
    "    \n",
    "    # Consider only interactions within the last 24 hours\n",
    "    recent_behaviors_df = behaviors_df[behaviors_df['Time'] >= most_recent_interaction_time - timedelta(days=1)]\n",
    "    \n",
    "    # Initialize a dictionary to hold weighted click counts\n",
    "    weighted_clicks = {}\n",
    "    \n",
    "    # Apply recency weighting to clicks\n",
    "    for index, row in recent_behaviors_df.iterrows():\n",
    "        interaction_time = row['Time']\n",
    "        weight = (most_recent_interaction_time - interaction_time).total_seconds() / (24 * 3600)  # Normalize to a 0-1 scale based on 24-hour period\n",
    "        weight = 1 - weight  # Invert so recent interactions have higher weight\n",
    "        \n",
    "        for impression in row[\"Impressions\"].split(\" \"):\n",
    "            article_id, clicked = impression.split(\"-\")\n",
    "            if clicked == \"1\":\n",
    "                if article_id not in weighted_clicks:\n",
    "                    weighted_clicks[article_id] = 0\n",
    "                weighted_clicks[article_id] += weight\n",
    "    \n",
    "    # Convert dictionary to DataFrame for sorting\n",
    "    weighted_clicks_df = pd.DataFrame(list(weighted_clicks.items()), columns=['News ID', 'Weighted Clicks'])\n",
    "    \n",
    "    # Sort by weighted clicks to get the most popular articles\n",
    "    weighted_clicks_df = weighted_clicks_df.sort_values(by='Weighted Clicks', ascending=False)\n",
    "    \n",
    "    # Return top 10 most popular news articles based on recency-weighted clicks\n",
    "    return weighted_clicks_df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    News ID  Weighted Clicks\n",
      "32   N23446       707.192535\n",
      "33   N38779       574.013322\n",
      "46   N61233       559.266609\n",
      "129  N45523       556.067720\n",
      "30   N19661       493.098623\n",
      "8    N34185       446.595544\n",
      "59   N56211       390.775856\n",
      "90   N41934       388.134063\n",
      "24    N6837       335.275995\n",
      "6     N6477       325.801343\n"
     ]
    }
   ],
   "source": [
    "# Find and print the most popular news articles\n",
    "#most_popular_news = find_most_popular_news(behaviors_df=behaviors_train_df, n=10)\n",
    "#print(most_popular_news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
