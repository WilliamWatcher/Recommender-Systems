{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based Collaborative Filtering\n",
    "\n",
    "## PERSONAL NOTES:\n",
    "Runnning with pyspark\n",
    "- If you get Py4JJavaError, remember to ensure pyspark system variables correctly\n",
    "    - echo %PYSPARK_PYTHON%\n",
    "    - echo %PYSPARK_DRIVER_PYTHON%\n",
    "\n",
    "\n",
    "#### Rationale\n",
    "1. Relatively large number of users compared to relevant news articles. Thus it is easier computationally to compare items than users.\n",
    "2. Item stability > User stability. Once a news article is out, it's content is fixed, while a user might change taste often. This can make user-based collaborative filtering more inaccurate in relation to the user's present taste. Similarity between items is constanst, i.e. the need for recalculations will be less with item-based collaborative filtering.\n",
    "3. Few news article interactions per user. This makes it harder to guess similar users as in user-based collaborative filtering.\n",
    "\n",
    "\n",
    "#### Item-based collaborative filtering in a nutshell (MIND)\n",
    "\"Find articles that are likely to be of interest, based on shared user interest patterns. Return the top N articles that are most similar to any of the news articles the user has clicked on, based on the similarity calculations between items.\"\n",
    "1. For each news article a user has clicked, get an overview of articles other users have also clicked\n",
    "2. Matrix factorization for efficiency - Alternating Least Squares (ALS)\n",
    "3. Calculate the similarity of each article (similarity of interactions) - Locality-Sensitive Hasing (LSH)\n",
    "4. Repeat steps for each news articles, and sort the recommendations list according to articles with the highest cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|Time                  |History                                                                                                                                                                                                                                                                                                  |Impressions                                                                                                                                                                                                                        |\n",
      "+------------+------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1           |U80234|11/15/2019 12:37:50 PM|N55189 N46039 N51741 N53234 N11276 N264 N40716 N28088 N43955 N6616 N47686 N63573 N38895 N30924 N35671   N28682-0 N48740-0 N31958-1 N34130-0 N6916-0 N5472-0 N50775-0 N24802-0 N19990-0 N33176-0 N62365-0 N5940-0 N6400-0 N58098-0 N42844-0 N49285-0 N51470-0 N53572-0 N11930-0 N21679-0 N55237-0 N29862-0|NULL                                                                                                                                                                                                                               |\n",
      "|2           |U60458|11/15/2019 7:11:50 AM |N58715 N32109 N51180 N33438 N54827 N28488 N61186 N34775 N33742 N50020 N57061 N30924 N6778                                                                                                                                                                                                                |N20036-0 N23513-1 N32536-0 N46976-0 N35216-0 N36779-0 N31958-0                                                                                                                                                                     |\n",
      "|3           |U44190|11/15/2019 9:55:12 AM |N56253 N1150 N55189 N16233 N61704 N51706 N53033 N15634 N3259                                                                                                                                                                                                                                             |N36779-0 N62365-0 N58098-0 N5472-0 N13408-0 N55036-0 N19990-0 N53283-0 N20036-0 N47383-0 N37352-0 N31958-0 N50775-0 N5940-1 N58251-0 N49285-0 N30290-0 N11930-0 N16680-0 N42844-0 N53572-0 N6916-0 N55237-0                        |\n",
      "|4           |U87380|11/15/2019 3:12:46 PM |N63554 N49153 N28678 N23232 N43369 N58518 N44402 N7649 N63429 N45794 N53531 N53033 N30765 N34452 N24298 N29361 N2597 N28926 N28247                                                                                                                                                                       |N6950-0 N60215-0 N6074-0 N11930-0 N6916-0 N24802-0 N48740-0 N60675-0 N45057-0 N51470-0 N62365-0 N15347-1 N21941-0 N49285-0 N29091-0 N6400-0 N19611-0 N52492-0 N29862-0 N19990-0 N38620-0 N60762-0 N34130-0 N1952-0 N53572-0 N4733-0|\n",
      "|5           |U9444 |11/15/2019 8:25:46 AM |N51692 N18285 N26015 N22679 N55556                                                                                                                                                                                                                                                                       |N5940-1 N23513-0 N49285-0 N23355-0 N19990-0 N31958-1 N29393-0 N30290-0 N19611-0 N62365-0 N51470-0 N34130-0 N36779-0 N20036-0                                                                                                       |\n",
      "+------------+------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------+\n",
      "|user_id|news_article|\n",
      "+-------+------------+\n",
      "|U80234 |N55189      |\n",
      "|U80234 |N46039      |\n",
      "|U80234 |N51741      |\n",
      "|U80234 |N53234      |\n",
      "|U80234 |N11276      |\n",
      "+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import explode, split, col, lit, desc, sum, udf, broadcast\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MINDItemBasedFiltering\") \\\n",
    "    .getOrCreate()\n",
    "#.config(\"spark.driver.memory\", \"4g\") \\\n",
    "    \n",
    "# Define the schema of the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"ImpressionID\", IntegerType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"History\", StringType(), True),\n",
    "    StructField(\"Impressions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the dataset with the defined schema\n",
    "data = spark.read.csv(\"data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", schema=schema)\n",
    "\n",
    "data.show(5, truncate=False)\n",
    "\n",
    "# Explode the history column into separate rows for each article per user \n",
    "# I.e. UserID | NewsArticle (that that user has stored in their history)\n",
    "data = data.withColumn(\"NewsArticle\", explode(split(col(\"History\"), \" \"))) \\\n",
    "    .select(col(\"UserID\").alias(\"user_id\"), col(\"NewsArticle\").alias(\"news_article\"))\n",
    "\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+------+\n",
      "|user_id_index|news_article_id_index|rating|\n",
      "+-------------+---------------------+------+\n",
      "|10460.0      |6.0                  |1     |\n",
      "|10460.0      |279.0                |1     |\n",
      "|10460.0      |1243.0               |1     |\n",
      "|10460.0      |201.0                |1     |\n",
      "|10460.0      |1734.0               |1     |\n",
      "+-------------+---------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[-0.16858101, 0.0...|\n",
      "| 10|[0.35777473, -0.5...|\n",
      "| 20|[0.08643527, -0.3...|\n",
      "| 30|[0.17135058, 0.15...|\n",
      "| 40|[0.13522969, 0.14...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare data to work with ALS\n",
    "# Add a dummy 'rating' column to indicate interaction\n",
    "data = data.withColumn(\"rating\", lit(1))\n",
    "\n",
    "# Index the user_id and news_article columns\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_index\").fit(data)\n",
    "item_indexer = StringIndexer(inputCol=\"news_article\", outputCol=\"news_article_id_index\").fit(data)\n",
    "\n",
    "data = user_indexer.transform(data)\n",
    "data = item_indexer.transform(data)\n",
    "\n",
    "# Extract mappings from StringIndexer models\n",
    "user_id_index_mapping = user_indexer.labels\n",
    "news_article_id_index_mapping = item_indexer.labels\n",
    "\n",
    "# Convert mappings to DataFrames for easier use\n",
    "user_id_index_df = spark.createDataFrame([(i, user_id_index_mapping[i]) for i in range(len(user_id_index_mapping))], [\"user_id_index\", \"user_id\"])\n",
    "news_article_id_index_df = spark.createDataFrame([(i, news_article_id_index_mapping[i]) for i in range(len(news_article_id_index_mapping))], [\"news_article_id_index\", \"news_article\"])\n",
    "\n",
    "# Select the final columns for ALS\n",
    "data = data.select(\"user_id_index\", \"news_article_id_index\", \"rating\")\n",
    "\n",
    "data.show(5, truncate=False)\n",
    "\n",
    "# Train the ALS model\n",
    "# Note: We use implicitPrefs=True to indicate that we are working with implicit feedback (clicks)\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_index\", itemCol=\"news_article_id_index\", ratingCol=\"rating\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "model = als.fit(data)\n",
    "\n",
    "# Extract the item factors from the ALS model\n",
    "#item_factors = model.itemFactors\n",
    "item_factors = model.itemFactors.limit(100) #Subset for testing\n",
    "\n",
    "item_factors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Similarity - Locality-Sensitive Hasing (LSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "|idA|idB| EuclideanDistance|\n",
      "+---+---+------------------+\n",
      "|  0|990|1.4072517684549435|\n",
      "|  0|930|1.4205228862444568|\n",
      "|  0|910|1.4034149854427065|\n",
      "|  0|900| 1.300589241076896|\n",
      "|  0|880|1.4031998195466355|\n",
      "|  0|870|1.3720967811574778|\n",
      "|  0|850|1.3959277875122378|\n",
      "|  0|840|1.3732905159559319|\n",
      "|  0|830|  1.31786995895055|\n",
      "|  0|820|1.3897629722269176|\n",
      "|  0|810|1.3124629287894602|\n",
      "|  0|800|1.3907342338714066|\n",
      "|  0|780|1.3034421385096169|\n",
      "|  0|760|1.4344558859378924|\n",
      "|  0|750|1.3773762779332417|\n",
      "|  0|740|1.4013291473390703|\n",
      "|  0|730|1.4315972423762529|\n",
      "|  0|720|1.4093930188192254|\n",
      "|  0|710|1.4241073268436972|\n",
      "|  0|700|1.3347027024321076|\n",
      "+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In order to calculate the similarity between items by using Spark's LSH, we need to convert the item factors into a DenseVector\n",
    "# Define a UDF that converts an array of floats into a DenseVector\n",
    "to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "# Apply the UDF to the 'features' column\n",
    "item_factors = item_factors.withColumn(\"features\", to_vector(\"features\"))\n",
    "\n",
    "# Prepare for calculating similarity\n",
    "# Initialize the LSH model\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0, numHashTables=3)\n",
    "\n",
    "# Fit the LSH model on the item factors\n",
    "model_lsh = brp.fit(item_factors)\n",
    "\n",
    "# Transform item factors to hash table\n",
    "item_factors_hashed = model_lsh.transform(item_factors)\n",
    "\n",
    "# Calculate Similiary\n",
    "# Calculate approx similarity join\n",
    "similar_items = model_lsh.approxSimilarityJoin(item_factors_hashed, item_factors_hashed, threshold=1.5, distCol=\"EuclideanDistance\")\n",
    "\n",
    "# Show some results\n",
    "similar_items.select(col(\"datasetA.id\").alias(\"idA\"), col(\"datasetB.id\").alias(\"idB\"), \"EuclideanDistance\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare similiarity data for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_interactions = data.select(\"user_id_index\", \"news_article_id_index\").distinct()\n",
    "\n",
    "# Step 1: Flatten similar_items for easier handling\n",
    "# You might need to adjust this part based on your exact schema of similar_items\n",
    "flat_similar_items = similar_items.select(\n",
    "    col(\"datasetA.id\").alias(\"article_id\"),\n",
    "    col(\"datasetB.id\").alias(\"similar_article_id\"),\n",
    "    col(\"EuclideanDistance\")\n",
    ")\n",
    "\n",
    "# Step 2: Filter for new recommendations per user\n",
    "# Join user interactions with similar items to find potential recommendations\n",
    "potential_recommendations = user_item_interactions.join(\n",
    "    broadcast(flat_similar_items),\n",
    "    user_item_interactions.news_article_id_index == flat_similar_items.article_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    \"user_id_index\",\n",
    "    \"similar_article_id\",\n",
    "    \"EuclideanDistance\"\n",
    ").distinct()\n",
    "\n",
    "# Step 3: Filter out articles the user has already interacted with\n",
    "filtered_recommendations = potential_recommendations.join(\n",
    "    broadcast(user_item_interactions),\n",
    "    (potential_recommendations.user_id_index == user_item_interactions.user_id_index) & \n",
    "    (potential_recommendations.similar_article_id == user_item_interactions.news_article_id_index),\n",
    "    \"left_anti\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found user_id_index 10460 for user_id U80234\n",
      "+------------------+------------+------------------+\n",
      "|similar_article_id|news_article|             score|\n",
      "+------------------+------------+------------------+\n",
      "|                50|      N26026|1.9272633605596365|\n",
      "|               160|      N18094|1.8533538436282824|\n",
      "|               240|       N3046|1.6369071560230017|\n",
      "|               440|      N63855|1.6698685785949838|\n",
      "|               960|      N25677|1.7231495800966772|\n",
      "+------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_recommendations(user_id, N=5):\n",
    "    # Check if the user_id mapping to user_id_index is successfull and the userID exists in the dataset\n",
    "    user_id_index_row = user_id_index_df.filter(col(\"user_id\") == user_id).select(\"user_id_index\").first()\n",
    "    if user_id_index_row is None:\n",
    "        print(f\"No user_id_index found for user_id {user_id}\")\n",
    "        return None\n",
    "    user_id_index = user_id_index_row[\"user_id_index\"]\n",
    "    print(f\"Found user_id_index {user_id_index} for user_id {user_id}\")\n",
    "    \n",
    "    # Filter for recommendations specific to this user_id_index\n",
    "    specific_user_recommendations = filtered_recommendations.filter(\n",
    "        filtered_recommendations.user_id_index == user_id_index\n",
    "    )\n",
    "    #specific_user_recommendations.show()\n",
    "\n",
    "    \n",
    "    # Aggregate and rank recommendations for the user\n",
    "    ranked_recommendations = specific_user_recommendations.groupBy(\"similar_article_id\").agg(\n",
    "        (1 / sum(\"EuclideanDistance\")).alias(\"score\")\n",
    "    ).orderBy(desc(\"score\"))\n",
    "\n",
    "    \n",
    "    # Fetch the top N recommendations\n",
    "    top_n_recommendations = ranked_recommendations.limit(N)\n",
    "    #top_n_recommendations.show()\n",
    "    \n",
    "    # Alias the DataFrames to clearly distinguish between them in the join condition\n",
    "    top_n_recommendations_alias = top_n_recommendations.alias(\"top_n\")\n",
    "    news_article_id_index_df_alias = news_article_id_index_df.alias(\"article_id_index\")\n",
    "\n",
    "    # Perform the join using the aliased DataFrames\n",
    "    top_n_recommendations_mapped = top_n_recommendations_alias.join(\n",
    "        news_article_id_index_df_alias, \n",
    "        col(\"top_n.similar_article_id\") == col(\"article_id_index.news_article_id_index\")  # Use the aliased column references\n",
    "    )\n",
    "\n",
    "    # Select the desired columns from the joined DataFrame\n",
    "    top_n_recommendations_mapped = top_n_recommendations_mapped.select(\n",
    "        col(\"top_n.similar_article_id\"), \n",
    "        col(\"article_id_index.news_article\"), \n",
    "        col(\"top_n.score\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    return top_n_recommendations_mapped\n",
    "\n",
    "# Usage example:\n",
    "top_n_recommendations = get_top_n_recommendations(\"U80234\", 5)\n",
    "top_n_recommendations.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
