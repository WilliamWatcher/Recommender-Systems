{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based Collaborative Filtering\n",
    "\n",
    "## PERSONAL NOTES:\n",
    "Runnning with pyspark\n",
    "- If you get Py4JJavaError, remember to ensure pyspark system variables correctly\n",
    "    - echo %PYSPARK_PYTHON%\n",
    "    - echo %PYSPARK_DRIVER_PYTHON%\n",
    "\n",
    "\n",
    "#### Rationale\n",
    "1. Relatively large number of users compared to relevant news articles. Thus it is easier computationally to compare items than users.\n",
    "2. Item stability > User stability. Once a news article is out, it's content is fixed, while a user might change taste often. This can make user-based collaborative filtering more inaccurate in relation to the user's present taste. Similarity between items is constanst, i.e. the need for recalculations will be less with item-based collaborative filtering.\n",
    "3. Few news article interactions per user. This makes it harder to guess similar users as in user-based collaborative filtering.\n",
    "\n",
    "\n",
    "#### Item-based collaborative filtering in a nutshell (MIND)\n",
    "\"Find articles that are likely to be of interest, based on shared user interest patterns. Return the top N articles that are most similar to any of the news articles the user has clicked on, based on the similarity calculations between items.\"\n",
    "1. For each news article a user has clicked, get an overview of articles other users have also clicked\n",
    "2. Matrix factorization for efficiency\n",
    "3. Calculate the cosine similarity of each article - i.e. the article most often interacted with together with the initial one\n",
    "4. Repeat steps for each news articles, and sort the recommendations list according to articles with the highest cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|Time                  |History                                                                                                                                                                                                                                                                                                  |Impressions                                                                                                                                                                                                                        |\n",
      "+------------+------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1           |U80234|11/15/2019 12:37:50 PM|N55189 N46039 N51741 N53234 N11276 N264 N40716 N28088 N43955 N6616 N47686 N63573 N38895 N30924 N35671   N28682-0 N48740-0 N31958-1 N34130-0 N6916-0 N5472-0 N50775-0 N24802-0 N19990-0 N33176-0 N62365-0 N5940-0 N6400-0 N58098-0 N42844-0 N49285-0 N51470-0 N53572-0 N11930-0 N21679-0 N55237-0 N29862-0|NULL                                                                                                                                                                                                                               |\n",
      "|2           |U60458|11/15/2019 7:11:50 AM |N58715 N32109 N51180 N33438 N54827 N28488 N61186 N34775 N33742 N50020 N57061 N30924 N6778                                                                                                                                                                                                                |N20036-0 N23513-1 N32536-0 N46976-0 N35216-0 N36779-0 N31958-0                                                                                                                                                                     |\n",
      "|3           |U44190|11/15/2019 9:55:12 AM |N56253 N1150 N55189 N16233 N61704 N51706 N53033 N15634 N3259                                                                                                                                                                                                                                             |N36779-0 N62365-0 N58098-0 N5472-0 N13408-0 N55036-0 N19990-0 N53283-0 N20036-0 N47383-0 N37352-0 N31958-0 N50775-0 N5940-1 N58251-0 N49285-0 N30290-0 N11930-0 N16680-0 N42844-0 N53572-0 N6916-0 N55237-0                        |\n",
      "|4           |U87380|11/15/2019 3:12:46 PM |N63554 N49153 N28678 N23232 N43369 N58518 N44402 N7649 N63429 N45794 N53531 N53033 N30765 N34452 N24298 N29361 N2597 N28926 N28247                                                                                                                                                                       |N6950-0 N60215-0 N6074-0 N11930-0 N6916-0 N24802-0 N48740-0 N60675-0 N45057-0 N51470-0 N62365-0 N15347-1 N21941-0 N49285-0 N29091-0 N6400-0 N19611-0 N52492-0 N29862-0 N19990-0 N38620-0 N60762-0 N34130-0 N1952-0 N53572-0 N4733-0|\n",
      "|5           |U9444 |11/15/2019 8:25:46 AM |N51692 N18285 N26015 N22679 N55556                                                                                                                                                                                                                                                                       |N5940-1 N23513-0 N49285-0 N23355-0 N19990-0 N31958-1 N29393-0 N30290-0 N19611-0 N62365-0 N51470-0 N34130-0 N36779-0 N20036-0                                                                                                       |\n",
      "+------------+------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------+\n",
      "|user_id|news_article|\n",
      "+-------+------------+\n",
      "|U80234 |N55189      |\n",
      "|U80234 |N46039      |\n",
      "|U80234 |N51741      |\n",
      "|U80234 |N53234      |\n",
      "|U80234 |N11276      |\n",
      "+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MINDItemBasedFiltering\") \\\n",
    "    .getOrCreate()\n",
    "#.config(\"spark.driver.memory\", \"4g\") \\\n",
    "    \n",
    "# Define the schema of the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"ImpressionID\", IntegerType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"History\", StringType(), True),\n",
    "    StructField(\"Impressions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the dataset with the defined schema\n",
    "data = spark.read.csv(\"data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", schema=schema)\n",
    "\n",
    "data.show(5, truncate=False)\n",
    "\n",
    "# Explode the history column into separate rows for each article per user \n",
    "# I.e. UserID | NewsArticle (that that user has stored in their history)\n",
    "data = data.withColumn(\"NewsArticle\", explode(split(col(\"History\"), \" \"))) \\\n",
    "    .select(col(\"UserID\").alias(\"user_id\"), col(\"NewsArticle\").alias(\"news_article\"))\n",
    "\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+------+\n",
      "|user_id_index|news_article_id_index|rating|\n",
      "+-------------+---------------------+------+\n",
      "|10460.0      |6.0                  |1     |\n",
      "|10460.0      |279.0                |1     |\n",
      "|10460.0      |1243.0               |1     |\n",
      "|10460.0      |201.0                |1     |\n",
      "|10460.0      |1734.0               |1     |\n",
      "+-------------+---------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[-0.061971296, 0....|\n",
      "| 10|[-0.41230685, -0....|\n",
      "| 20|[-0.38864496, -0....|\n",
      "| 30|[0.06233177, -0.3...|\n",
      "| 40|[0.060754064, -0....|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Prepare data to work with ALS\n",
    "# Add a dummy 'rating' column to indicate interaction\n",
    "data = data.withColumn(\"rating\", lit(1))\n",
    "\n",
    "# Index the user_id and news_article columns\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_index\").fit(data)\n",
    "item_indexer = StringIndexer(inputCol=\"news_article\", outputCol=\"news_article_id_index\").fit(data)\n",
    "\n",
    "data = user_indexer.transform(data)\n",
    "data = item_indexer.transform(data)\n",
    "\n",
    "# Select the final columns for ALS\n",
    "data = data.select(\"user_id_index\", \"news_article_id_index\", \"rating\")\n",
    "\n",
    "data.show(5, truncate=False)\n",
    "\n",
    "# Train the ALS model\n",
    "# Note: We use implicitPrefs=True to indicate that we are working with implicit feedback (clicks)\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_index\", itemCol=\"news_article_id_index\", ratingCol=\"rating\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "model = als.fit(data)\n",
    "\n",
    "# Extract the item factors from the ALS model\n",
    "#item_factors = model.itemFactors\n",
    "item_factors = model.itemFactors.limit(100) #Subset for testing\n",
    "\n",
    "item_factors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Similarity - Locality-Sensitive Hasing (LSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|value|value_plus_one|\n",
      "+-----+--------------+\n",
      "|    1|             2|\n",
      "|    2|             3|\n",
      "|    3|             4|\n",
      "+-----+--------------+\n",
      "\n",
      "+---+---+------------------+\n",
      "|idA|idB| EuclideanDistance|\n",
      "+---+---+------------------+\n",
      "|  0|960|1.3926048418531423|\n",
      "|  0|940|1.4512176058822914|\n",
      "|  0|880|1.4496000348835834|\n",
      "|  0|860| 1.423034523740376|\n",
      "|  0|850| 1.405332551859079|\n",
      "|  0|840|1.4391240727569816|\n",
      "|  0|820| 1.420895901990328|\n",
      "|  0|790|1.4255602183109615|\n",
      "|  0|770|1.3928357104564038|\n",
      "|  0|750|1.4284613302307814|\n",
      "|  0|690|1.4973369852231517|\n",
      "|  0|680|1.3939049903992624|\n",
      "|  0|620|1.4236844674251254|\n",
      "|  0|600|1.3559688234355236|\n",
      "|  0|590|1.4914945837604392|\n",
      "|  0|560|1.3945500040535608|\n",
      "|  0|540|1.4818432062614648|\n",
      "|  0|520|1.4655622544172267|\n",
      "|  0|510|1.4641489461645332|\n",
      "|  0|490|  1.45676267349765|\n",
      "+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# In order to calculate the similarity between items by using Spark's LSH, we need to convert the item factors into a DenseVector\n",
    "# Define a UDF that converts an array of floats into a DenseVector\n",
    "to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "# Apply the UDF to the 'features' column\n",
    "item_factors = item_factors.withColumn(\"features\", to_vector(\"features\"))\n",
    "\n",
    "# Prepare for calculating similarity\n",
    "# Initialize the LSH model\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0, numHashTables=3)\n",
    "\n",
    "# Fit the LSH model on the item factors\n",
    "model_lsh = brp.fit(item_factors)\n",
    "\n",
    "# Transform item factors to hash table\n",
    "item_factors_hashed = model_lsh.transform(item_factors)\n",
    "\n",
    "# Calculate Similiary\n",
    "# Calculate approx similarity join\n",
    "similar_items = model_lsh.approxSimilarityJoin(item_factors_hashed, item_factors_hashed, threshold=1.5, distCol=\"EuclideanDistance\")\n",
    "\n",
    "# Show some results\n",
    "similar_items.select(col(\"datasetA.id\").alias(\"idA\"), col(\"datasetB.id\").alias(\"idB\"), \"EuclideanDistance\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
