{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based Collaborative Filtering\n",
    "\n",
    "#### Rationale\n",
    "1. Relatively large number of users compared to relevant news articles. Thus it is easier computationally to compare items than users.\n",
    "2. Item stability > User stability. Once a news article is out, it's content is fixed, while a user might change taste often. This can make user-based collaborative filtering more inaccurate in relation to the user's present taste. Similarity between items is constanst, i.e. the need for recalculations will be less with item-based collaborative filtering.\n",
    "3. Few news article interactions per user. This makes it harder to guess similar users as in user-based collaborative filtering.\n",
    "\n",
    "\n",
    "#### Item-based collaborative filtering in a nutshell (MIND)\n",
    "\"Find articles that are likely to be of interest, based on shared user interest patterns. Return the top N articles that are most similar to any of the news articles the user has clicked on, based on the similarity calculations between items.\"\n",
    "1. For each news article a user has clicked, get an overview of articles other users have also clicked\n",
    "2. Matrix factorization for efficiency - Alternating Least Squares (ALS)\n",
    "3. Calculate the similarity of each article (similarity of interactions) - Locality-Sensitive Hasing (LSH)\n",
    "4. Repeat steps for each news articles, and sort the recommendations list according to articles with the highest cosine similarity\n",
    "\n",
    "\n",
    "##### If error runnning with pyspark:\n",
    "- If you get Py4JJavaError, remember to ensure pyspark system variables correctly\n",
    "    - echo %PYSPARK_PYTHON%\n",
    "    - echo %PYSPARK_DRIVER_PYTHON%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preperation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marle\\AppData\\Local\\Temp\\ipykernel_23900\\1531208401.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import explode, split, col, lit, desc, sum, udf, broadcast\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MINDItemBasedFiltering\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def prepare_data_with_spark_df(pandas_df):\n",
    "    # Define the schema of the dataset as in your previous code\n",
    "    schema = StructType([\n",
    "        StructField(\"ImpressionID\", IntegerType(), True),\n",
    "        StructField(\"UserID\", StringType(), True),\n",
    "        StructField(\"Time\", StringType(), True),\n",
    "        StructField(\"History\", StringType(), True),\n",
    "        StructField(\"Impressions\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Convert the pandas dataframe to a spark dataframe using the defined schema\n",
    "    spark_df = spark.createDataFrame(pandas_df, schema=schema)\n",
    "    \n",
    "    # Process the dataframe similarly to how you've done with the TSV file\n",
    "    # Explode the history column into separate rows for each article per user\n",
    "    spark_df = spark_df.withColumn(\"NewsArticle\", explode(split(col(\"History\"), \" \"))) \\\n",
    "                       .select(col(\"UserID\").alias(\"user_id\"), col(\"NewsArticle\").alias(\"news_article\"))\n",
    "    \n",
    "    # Return the processed Spark DataFrame\n",
    "    return spark_df\n",
    "\n",
    "#data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating_least_squares(data):\n",
    "    # Add a 'rating' column to indicate interaction\n",
    "    data = data.withColumn(\"rating\", lit(1))\n",
    "\n",
    "    # Index the user_id and news_article columns\n",
    "    user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_index\").fit(data)\n",
    "    item_indexer = StringIndexer(inputCol=\"news_article\", outputCol=\"news_article_id_index\").fit(data)\n",
    "\n",
    "    # Transform data with indexers\n",
    "    data = user_indexer.transform(data)\n",
    "    data = item_indexer.transform(data)\n",
    "\n",
    "    # Extract mappings from StringIndexer models\n",
    "    user_id_index_mapping = user_indexer.labels\n",
    "    news_article_id_index_mapping = item_indexer.labels\n",
    "\n",
    "    # Convert mappings to DataFrames for easier use\n",
    "    user_id_index_df = spark.createDataFrame(\n",
    "        [(i, user_id_index_mapping[i]) for i in range(len(user_id_index_mapping))],\n",
    "        [\"user_id_index\", \"user_id\"]\n",
    "    )\n",
    "    news_article_id_index_df = spark.createDataFrame(\n",
    "        [(i, news_article_id_index_mapping[i]) for i in range(len(news_article_id_index_mapping))],\n",
    "        [\"news_article_id_index\", \"news_article\"]\n",
    "    )\n",
    "\n",
    "    # Select the final columns for ALS\n",
    "    data = data.select(\"user_id_index\", \"news_article_id_index\", \"rating\")\n",
    "\n",
    "    # Train the ALS model\n",
    "    als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_index\", itemCol=\"news_article_id_index\", \n",
    "              ratingCol=\"rating\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "    model = als.fit(data)\n",
    "\n",
    "    # Extract the item factors from the ALS model and limit to 100 for testing\n",
    "    item_factors = model.itemFactors.limit(1000)\n",
    "    num_item_factors = item_factors.count()\n",
    "    print(f\"Number of item factors: {num_item_factors}\")\n",
    "\n",
    "    # Return the data, user_id_index_df, news_article_id_index_df, and the model for further use\n",
    "    return data, user_id_index_df, news_article_id_index_df, item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculating Similarity - Locality-Sensitive Hasing (LSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(item_factors):\n",
    "    # Define a UDF that converts an array of floats into a DenseVector\n",
    "    to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "    # Apply the UDF to the 'features' column\n",
    "    item_factors = item_factors.withColumn(\"features\", to_vector(\"features\"))\n",
    "\n",
    "    # Initialize the LSH model\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=3.0, numHashTables=2)\n",
    "\n",
    "    # Fit the LSH model on the item factors\n",
    "    model_lsh = brp.fit(item_factors)\n",
    "\n",
    "    # Transform item factors to hash table\n",
    "    item_factors_hashed = model_lsh.transform(item_factors)\n",
    "\n",
    "    # Calculate approximate similarity join\n",
    "    similar_items = model_lsh.approxSimilarityJoin(item_factors_hashed, item_factors_hashed, threshold=1.5, distCol=\"EuclideanDistance\")\n",
    "\n",
    "    # Select the relevant columns to clarify the output\n",
    "    similar_items = similar_items.select(\n",
    "        col(\"datasetA.id\").alias(\"idA\"), \n",
    "        col(\"datasetB.id\").alias(\"idB\"), \n",
    "        col(\"EuclideanDistance\")\n",
    "    )\n",
    "    \n",
    "    # Show some results\n",
    "    # For some reason, the line below makes the program run seemingly indefinitely\n",
    "    #similar_items.select(col(\"datasetA.id\").alias(\"idA\"), col(\"datasetB.id\").alias(\"idB\"), \"EuclideanDistance\").show(5)    \n",
    "\n",
    "    return similar_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare similiarity data for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_recommendations(data, similar_items, user_id_index_df, news_article_id_index_df):\n",
    "    # Step 1: Flatten similar_items for easier handling\n",
    "    # Correctly reference the nested columns\n",
    "    flat_similar_items = similar_items.select(\n",
    "        col(\"idA\").alias(\"article_id\"),\n",
    "        col(\"idB\").alias(\"similar_article_id\"),\n",
    "        col(\"EuclideanDistance\")\n",
    "    )\n",
    "    \n",
    "    # Get distinct user-item interactions\n",
    "    user_item_interactions = data.select(\"user_id_index\", \"news_article_id_index\").distinct().cache()\n",
    "\n",
    "    # Step 2: Filter for new recommendations per user\n",
    "    # Join user interactions with similar items to find potential recommendations\n",
    "    potential_recommendations = user_item_interactions.join(\n",
    "        broadcast(flat_similar_items),\n",
    "        user_item_interactions.news_article_id_index == flat_similar_items.article_id,\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        \"user_id_index\",\n",
    "        \"similar_article_id\",\n",
    "        \"EuclideanDistance\"\n",
    "    ).distinct()\n",
    "\n",
    "    # Step 3: Filter out articles the user has already interacted with\n",
    "    filtered_recommendations = potential_recommendations.join(\n",
    "        broadcast(user_item_interactions),\n",
    "        (potential_recommendations.user_id_index == user_item_interactions.user_id_index) & \n",
    "        (potential_recommendations.similar_article_id == user_item_interactions.news_article_id_index),\n",
    "        \"left_anti\"\n",
    "    )\n",
    "\n",
    "    # Join with user_id_index_df to convert user_id_index back to user_id\n",
    "    filtered_recommendations = filtered_recommendations.join(\n",
    "        broadcast(user_id_index_df),\n",
    "        filtered_recommendations.user_id_index == user_id_index_df.user_id_index\n",
    "    )\n",
    "\n",
    "    # Join with news_article_id_index_df to convert similar_article_id back to news_article\n",
    "    filtered_recommendations = filtered_recommendations.join(\n",
    "        broadcast(news_article_id_index_df),\n",
    "        filtered_recommendations.similar_article_id == news_article_id_index_df.news_article_id_index\n",
    "    )\n",
    "\n",
    "    # Select the original user and news article IDs, along with the EuclideanDistance\n",
    "    filtered_recommendations = filtered_recommendations.select(\n",
    "        col(\"user_id\"), \n",
    "        col(\"news_article\"),\n",
    "        col(\"EuclideanDistance\")\n",
    "    )\n",
    "    \n",
    "    return filtered_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Execution) Train collaborative filtering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collaborative_filtering_model(pandas_df):\n",
    "    # Prepare the data as a Spark DataFrame\n",
    "    data = prepare_data_with_spark_df(pandas_df)\n",
    "\n",
    "    # Train the ALS model\n",
    "    data, user_id_index_df, news_article_id_index_df, item_factors = alternating_least_squares(data)\n",
    "\n",
    "    # Calculate the similarity between items\n",
    "    similar_items = calculate_similarity(item_factors)\n",
    "\n",
    "    # Generate recommendations for all users\n",
    "    recommendations = all_recommendations(data, similar_items, user_id_index_df, news_article_id_index_df)\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(user_id, filtered_recommendations, N=5):\n",
    "    # Fetch recommendations for the specific user\n",
    "    specific_user_recommendations = filtered_recommendations.filter(\n",
    "        filtered_recommendations.user_id == user_id\n",
    "    )\n",
    "    \n",
    "    # Aggregate and rank recommendations\n",
    "    ranked_recommendations = specific_user_recommendations.groupBy(\"news_article\").agg(\n",
    "        (1 / sum(\"EuclideanDistance\")).alias(\"score\")\n",
    "    ).orderBy(desc(\"score\")).limit(N)\n",
    "    \n",
    "    return ranked_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Based Collaborative Filtering Method Example\n",
    "1. Trains the model\n",
    "2. Calculates and prints the top 5 recommendations for user \"U80234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of item factors: 100\n",
      "+------------+------------------+\n",
      "|news_article|             score|\n",
      "+------------+------------------+\n",
      "|      N64631| 1.732766934639083|\n",
      "|      N17109|1.6911987136771351|\n",
      "|      N57812|1.3562689985581646|\n",
      "|      N63276|1.0071215188203804|\n",
      "|      N56469|0.9443795341933542|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "pandas_df = pd.read_csv(\"data/MINDsmall_dev/behaviors.tsv\", sep=\"\\t\", header=None, names=[\"ImpressionID\", \"UserID\", \"Time\", \"History\", \"Impressions\"])\n",
    "filtered_recommendations = train_collaborative_filtering_model(pandas_df)\n",
    "ranked_recommendations = get_top_n_recommendations(\"U80234\",filtered_recommendations)\n",
    "ranked_recommendations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
